# -*- coding: utf-8 -*-
"""analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pl0btOfEBhiZA-FAya8tLiO41fCrQvn9
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install openai
# %pip install -U sentence-transformers

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from matplotlib import rcParams
from plotly.subplots import make_subplots
from sklearn.feature_extraction.text import CountVectorizer
import openai
from google.colab import userdata
from collections import Counter
from sentence_transformers import SentenceTransformer, util
from functools import cache
import time
from itertools import combinations
from collections import Counter
import numpy as np
import pandas as pd
import seaborn as sns

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')

!pip install --upgrade gdown
!gdown https://drive.google.com/uc?id=1JvyS_pf_JsHK1Jlgu1i3MeQx8r55gdW_

data_path = 'Dataset_10k.csv'
df = pd.read_csv(data_path)

"""## Exploratory Data Analysis"""

df.columns

df[['month', 'year']].value_counts()

df[['language']].value_counts().head(15)

df['source'].value_counts().head(15)

len(df['source'].value_counts())

"""## Data Cleaning"""

df['translated_title_cleaned'] = (df['translated_title']
                                  .str.rsplit(' -', 1).str[0]
                                  .str.replace(r'[^\w\s]', '', regex=True)
                                  .str.lower())

df['translated_title_cleaned'][30]

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return ' '.join(lemmatized_tokens)

df['filtered_title'] = df['translated_title_cleaned'].apply(preprocess_text)
df['filtered_title'][0]

"""## Data Sentiment"""

def get_sentiment(text, pos=True):
  analyzer = SentimentIntensityAnalyzer()
  scores = analyzer.polarity_scores(text)
  if pos:
    return scores['pos']
  else:
    return scores['neg']

df['pos'] = df['filtered_title'].apply(lambda x: get_sentiment(x, True))
df['neg'] = df['filtered_title'].apply(lambda x: get_sentiment(x, False))

df['pos'].describe()

df['neg'].describe()

df['date'] = pd.to_datetime(df['date'])
df['date']

# Use analytical NLP to show posiitve and negative sentiment over time
avg_pos = df.groupby('month')['pos'].mean().reset_index()
avg_neg = df.groupby('month')['neg'].mean().reset_index()
fig = make_subplots(rows=1, cols=2)
fig.add_scatter(x=avg_pos['month'], y=avg_pos['pos'], mode='lines', name='Positive Sentiment', row=1, col=1)
fig.add_scatter(x=avg_neg['month'], y=avg_neg['neg'], mode='lines', name='Negative Sentiment', row=1, col=2)
fig.update_layout(title='Positive and Negative Sentiment Scores Over Time', xaxis_title='Month', yaxis_title='Score')
fig.show()

"""In months August-November, positive and negative sentiments followed swaying, opposite trends. In May-July, both sentiment values were high."""

# Average sentiment by top source
top_15_sources = df['source'].value_counts().head(15).index
filtered_df = df[df['source'].isin(top_15_sources)]

# Find the average pos and neg scores aggregated by source
avg_scores = filtered_df.groupby('source')[['pos', 'neg']].mean().reset_index()

# Plotting
plt.figure(figsize=(12, 8))
palette = sns.color_palette("coolwarm", 2)
rcParams['font.family'] = 'DejaVu Sans'

plt.subplot(1, 2, 1)
sns.barplot(x='pos', y='source', data=avg_scores.sort_values(by='pos', ascending=False), palette=[palette[0]])
plt.title('Positive Sentiment for Top 15 Sources', fontsize=14)
plt.xlabel('Average Positive Sentiment Score', fontsize=12)
plt.ylabel('Source', fontsize=12)

plt.subplot(1, 2, 2)
sns.barplot(x='neg', y='source', data=avg_scores.sort_values(by='neg', ascending=False), palette=[palette[1]])
plt.title('Negative Sentiment for Top 15 Sources', fontsize=14)
plt.xlabel('Average Negative Sentiment Score', fontsize=12)
plt.ylabel('')

plt.tight_layout()
plt.show()

"""# Visualize text data"""

# Word cloud
text = " ".join(title for title in df.filtered_title)
wc = WordCloud(background_color='white').generate(text)
plt.imshow(wc)
plt.axis('off')

# Create a bag of words and see the most frequent words
def count_words(text):
  words = text.split()
  return Counter(set(words))

bag_of_words = df['filtered_title'].apply(count_words).sum()
bag_of_words = dict(sorted(bag_of_words.items(), key=lambda item: item[1], reverse=True))
bag_of_words

# Filter out top 3 ("ai", "artificial", "intelligence")
x = list(bag_of_words.keys())[3:18]  # Selecting only the top 10 x values
y = list(bag_of_words.values())[3:18]  # Corresponding y values for the top 10 x values

fig = go.Bar(
    x=x,
    y=y,
    marker=dict(
        color='rgb(158,202,225)',
        line=dict(
            color='rgb(8,48,107)',
            width=1.5,
        )
    ),
    opacity=0.8
)

layout = go.Layout(
    title='Top 15 Most Frequent Words',
    xaxis=dict(
        tickmode='linear',
        tickvals=np.arange(10),
        ticktext=x,
        automargin=True
    ),
    yaxis=dict(
        title='Frequency',
        tickfont=dict(
            size=14,
            color='rgb(107, 107, 107)'
        )
    ),
    showlegend=False,
    width=800  # Adjusted width for better visualization
)

fig = go.Figure(data=[fig], layout=layout)
fig.show()

"""# LLMs for Insight Generation"""

# Use GPT-4 Turbo to analyze text into categories

categories = (
    'fear-mongering',
    'hype',
    'innovation and development',
    'consumer applications',
    'business applications',
    'economic implications',
    'jobs and employment',
    'language models',
    'robotics',
    'image and vision models',
    'legal and ethical issues',
    'medical issues',
    'education',
    'environment',
    'security',
    'government and regulation',
    'social impact'
)

RUN = False

# get Google Colab secret key for OpenAI Key
client = openai.OpenAI(
    api_key=userdata.get('OPENAI_API_KEY')
)

def classify_headlines(headline, categories):
    prompt = (f'Classify the headline as related to categories {categories} stating True or False for each category, space separated.'
            f'Headline {headline}:\n\n')

    # Make a request to the ChatGPT 4 Turbo API
    response = client.chat.completions.create(
    messages=[
        {
            'role': 'user',
            'content': prompt
        }
      ],
        model="gpt-4-0125-preview",
        max_tokens=150,
        temperature=0
    )
    output_text = response.choices[0].message.content.strip()
    return output_text

@cache  # to avoid re-calling the API on same inputs
def classify_row(headline, categories, print_output=False):
    first = classify_headlines(headline, categories)
    results = [x == 'True' for x in first.split() if x in ['True', 'False']]
    if len(results) != len(categories):
        print("Warning on headline", headline)
        results = [False for _ in range(len(categories))]
    if print_output:
        for i, cat in enumerate(categories):
            print(cat, results[i])
        print(headline)
    return results

# Test chatgpt classification performance
results = classify_row(df['translated_title'][12], categories, print_output=True)

classify_row.cache_info()

def apply_fn():
    applied = df['translated_title'].apply(lambda x: classify_row(x, categories, print_output=True))
    return applied

applied = None
while applied is None and RUN:
    try:
        applied = apply_fn()  # Takes advantage of caching
    except:  # Sometimes Bad Request Error
        applied = None

applied

if RUN:
    df[[categories[i] for i in range(len(categories))]] = pd.DataFrame(applied.tolist(), index=df.index)

if RUN:
    df.to_csv('processed_df.csv')

    ## Save to drive
    # from google.colab import drive
    # drive.mount('/content/drive')
    !cp processed_df.csv /content/drive/MyDrive/RAISE/Data

"""Used Chatgpt 4 Turbo, as during tests we found it much more performant than chatgpt 3.5 turbo at classification tasks for the categories we gave it."""

!pip install --upgrade gdown
!gdown https://drive.google.com/uc?id=1KWKk0-zZnMgEuen42OufTA_ygxXmamEG

# Use processed_df.csv
processed_df = pd.read_csv('processed_df.csv')

"""# Clustering"""

model = SentenceTransformer('all-MiniLM-L6-v2')
titles = list(processed_df['filtered_title'])
encoded_titles = model.encode(titles, batch_size=64, show_progress_bar=True, convert_to_tensor=True)

clusters = util.community_detection(encoded_titles, min_community_size=30, threshold=0.60)
clusters_dict = {}
for i, cluster in enumerate(clusters):
  for sentence_id in cluster:
    if f'Cluster {i}' not in clusters_dict:
      clusters_dict[f'Cluster {i}'] = [titles[sentence_id]]
    else:
      clusters_dict[f'Cluster {i}'].append(titles[sentence_id])
clusters_dict

RUN2 = False

client = openai.OpenAI(
    api_key=userdata.get('OPENAI_API_KEY')
)

@cache
def describe_clusters(headlines):
    prompt = (f'Create a label for this list of article titles. \n{headlines}.\n Response Format: {"Label"}#{"Description"}')

    # Make a request to the ChatGPT 4 Turbo API
    response = client.chat.completions.create(
    messages=[
        {
            'role': 'user',
            'content': prompt
        }
      ],
        model="gpt-4-0125-preview",
        temperature=0
    )
    output_text = response.choices[0].message.content.strip()
    return output_text

if RUN2:
    descriptions = []
    for i in range(len(clusters)):
        headlines = clusters_dict[f'Cluster {i}']
        print(headlines)
        desc = describe_clusters(tuple(headlines))
        print(desc)
        descriptions.append(desc)

for i in range(len(descriptions)):
    descriptions[i] = descriptions[i].replace('Label: ', '').replace('Description: ', '#').replace(' # ', '#').replace('##', '#').replace('\n', '')

clusters_df = pd.DataFrame([s.split("#") for s in descriptions], columns=["Label", "Description"])

clusters_df['Count'] = [len(clusters_dict[f'Cluster {i}']) for i in range(len(clusters_dict))]



clusters_df

clusters_df.to_csv('clusters_df.csv')
!cp clusters_df.csv /content/drive/MyDrive/RAISE/Data

# To download saved copy
!gdown https://drive.google.com/uc?id=1LTSa6-6kVMlIXLkba4zl_oYZyOd_yPDv
clusters_df = pd.read_csv('clusters_df.csv')

"""# Analyze Clusters"""

# Plot average sentiment by category
plot_data = []
for category in categories:
    filtered_df = processed_df[processed_df[category]]
    avg_pos = filtered_df['pos'].mean()
    avg_neg = filtered_df['neg'].mean()
    plot_data.append({'Category': category, 'Type': 'pos', 'Value': avg_pos})
    plot_data.append({'Category': category, 'Type': 'neg', 'Value': avg_neg})

plot_df = pd.DataFrame(plot_data)

plt.figure(figsize=(10, 6))
sns.barplot(x='Category', y='Value', hue='Type', data=plot_df)
plt.title('Average Sentiment for Article Categories')
plt.xticks(rotation=90)
plt.xlabel('Category')
plt.ylabel('Average Sentiment')
sns.despine()
plt.tight_layout()
plt.show()

"""# Co-Occurence Matrix"""

matrix_df = processed_df[list(categories)].astype(int)

# Create a co-occurrence matrix
co_occurrence_matrix = matrix_df.T.dot(matrix_df).values

# Visualize the co-occurrence matrix
plt.figure(figsize=(12, 10))
sns.heatmap(co_occurrence_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=matrix_df.columns, yticklabels=matrix_df.columns)
plt.title("Co-occurrence Matrix")
plt.show()